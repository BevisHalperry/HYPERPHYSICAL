<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2019-12-02 Mon 02:19 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>HYPERPHYSICAL</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Bevis" />
<link rel="stylesheet" type="text/css" href="assets/styles/css/htmlize.css"/>
<link rel="stylesheet"type="text/css" href="assets/styles/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="assets/styles/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="assets/styles/js/readtheorg.js"></script> 
</head>
<body>
<div id="content">
<h1 class="title">HYPERPHYSICAL</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org0464ed9">Project HYPERPHYSICAL*</a>
<ul>
<li><a href="#sec:overview">Description and Overview</a>
<ul>
<li><a href="#org0957495">Intended User Experience</a></li>
</ul>
</li>
<li><a href="#sec:context">Context and Research</a>
<ul>
<li><a href="#orgab2e23e">Concepts and Terms</a></li>
<li><a href="#org1815965">Inspiration and Similar Projects</a></li>
</ul>
</li>
<li><a href="#orge173c5c">Technology and Fabrication</a>
<ul>
<li><a href="#org78969d5">Sensing Sub-System</a></li>
<li><a href="#org0eed8a2">Data Pipeline Sub-System</a></li>
<li><a href="#org7feb28d">Remote Ubication Expression Sub-System (RUESS)</a></li>
</ul>
</li>
<li><a href="#org647c990">Development and Planning</a>
<ul>
<li><a href="#orgbe2bb3e">Timeline and Milestones</a></li>
<li><a href="#org3bcb927">Potential Issues and Contingencies</a></li>
<li><a href="#org9b5394f">Required Knowledge</a></li>
<li><a href="#org050fcc6">Required Learning</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org0464ed9" class="outline-2">
<h2 id="org0464ed9">Project HYPERPHYSICAL*</h2>
<div class="outline-text-2" id="text-org0464ed9">
<p>
<b><i><i>ˌhaɪpəˈfɪzɪkəl</i></i> (*HYP</b>)erphysica (<b>E</b>)nviromental (<b>R</b>)emote (<b>P</b>)resence of (<b>HY</b>)drology, (<b>S</b>)pace, (<b>I</b>)ncalescence, (<b>C</b>)olour (<b>A</b>)nd (<b>L</b>)ight
</p>

<p>
Remotely gathering environmental and atmospheric data from a location and expressing the data in a tactile, playful and insightful way.
</p>
</div>

<div id="outline-container-orgc9be8c1" class="outline-3">
<h3 id="sec:overview"><a id="orgc9be8c1"></a>Description and Overview</h3>
<div class="outline-text-3" id="text-sec:overview">
<p>
This project proposes a system to explore the idea of Environmental Remote Presence or Environmental Telepresence.
In contrast to traditional remote presence where an individuals "presence" is transfered across a distance digitally usually through audio, video and occasionally haptic data, Environmental Remote Presence transfers an area or locations presence across a distance digitally not just through audio and video data but also through environmental data such as temprature, humidity, sky colour and visibility.
This presence could be unidirectional or omnidirectional in nature, HYPERPHYSICAL is focused on unidirectional in the intrest of development time and cost, but only requires horizontal scaling to make the presence transfer both ways. 
</p>

<p>
HYPERPHYSICAL will take the form of 3 interlinked subsystems:
</p>

<ul class="org-ul">
<li><b>Sensing Sub-System</b>. A ruggedised, self-sustaining and unobtrusive sensing platform using off-the-shelf electronics which will gather environmental data using a suite of sensors and wirelessly transmit that data to the&#x2026;</li>
<li><b>Data Pipeline Sub-System</b>. A cloud-based (hopefully self-hosted) system which will parse, sanitise, store/publish and prepare the data for analysis or expression on the&#x2026;</li>
<li><b>Remote Ubication Expression Sub-System</b>. A device which expresses the data in a tactile, playful and insightful way. This may consist of LED's expressing the colour of the sky,mini fog machines expressing humidity, fans expressing wind-speed and Peltier modules expressing temperature.</li>
</ul>
</div>

<div id="outline-container-org0957495" class="outline-4">
<h4 id="org0957495">Intended User Experience</h4>
<div class="outline-text-4" id="text-org0957495">
<p>
The first and foremost intention of HYPERPHYSICAL is to attempt to make the intangible "nature" of an remote environment tangible to a person through quantifying,transmitting and 
expressing the natural properties of an environment. With the goal of improving and strengthening our relationship to the natural world and to explore ecological hyper-objects such
as climate change and other effects of the modern Anthropocene. 
</p>

<p>
Secondarily it could be used as:
</p>
<ul class="org-ul">
<li>a tool for climate activism through the idea that without a connection to, or even just a reminder of, an non-urbanised environment it becomes harder to act in the best interests of ecology.</li>
<li>a tool for improving mental health through the idea that exposure to natural environments improves our mental well-being.</li>
<li>a tool for education to show through tactile and novel expression abstract values such as humidity, temperature and wind-speed.</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-orgcc9b9e0" class="outline-3">
<h3 id="sec:context"><a id="orgcc9b9e0"></a>Context and Research</h3>
<div class="outline-text-3" id="text-sec:context">
</div>

<div id="outline-container-orgab2e23e" class="outline-4">
<h4 id="orgab2e23e">Concepts and Terms</h4>
<div class="outline-text-4" id="text-orgab2e23e">
</div>
<ul class="org-ul">
<li><a id="org3df571c"></a>Remote Presence<br /></li>
<li><a id="orgf012051"></a>Ecological Dataism<br /></li>

<li><a id="orgd0ff76f"></a>Playful Architecture<br /></li>

<li><a id="orgce796a8"></a>Unobtrusive sensing<br />
<div class="outline-text-5" id="text-orgce796a8">
<ul class="org-ul">
<li>Reliability</li>
<li>Non-toxic</li>
</ul>
</div>
</li>
<li><a id="orgb2fd663"></a>Hyperobjects<br /></li>
<li><a id="org6facd86"></a>Environmental art<br /></li>
<li><a id="org37a91a6"></a>Environmental sensing<br /></li>
<li><a id="orgf126508"></a>Ecological Activism<br /></li>
</ul>
</div>


<div id="outline-container-org1815965" class="outline-4">
<h4 id="org1815965">Inspiration and Similar Projects</h4>
<div class="outline-text-4" id="text-org1815965">
</div>
<ul class="org-ul">
<li><a id="orgce50dd7"></a><a href="http://datawalking.com/">DATA WALKING</a><br />
<div class="outline-text-5" id="text-orgce50dd7">
<p>
A paper based analog collection system for ecological data facilitated through walking
</p>
</div>
</li>
<li><a id="org1544341"></a><a href="https://citizensense.net/">Home | Citizen Sense</a><br />
<div class="outline-text-5" id="text-org1544341">
<p>
A series of projects democratising and making accessable environmental data collection for Wild and Urban areas and Pollution monitoring.
</p>
</div>
</li>
<li><a id="org6d4ac2a"></a><a href="http://jones-bulley.com/living-symphonies/">Living Symphonies (Daniel Jones and James Bulley)</a><br />
<div class="outline-text-5" id="text-org6d4ac2a">
<p>
A project which augments a natural landscape with the sonification of flora and fauna.
</p>
</div>
</li>
<li><a id="orgf240928"></a><a href="http://jones-bulley.com/vespers/">Vespers (James Bulley and Daniel Jones)</a><br />
<div class="outline-text-5" id="text-orgf240928">
<p>
A sound installation that composes a musical score in real-time, drawn from the online activity of the United Kingdom
</p>
</div>
</li>
<li><a id="org83dd556"></a><a href="http://www.sara-rodrigues.com/degrees-of-abstraction.html">Degrees of Abstraction - SARA RODRIGUES</a><br />
<div class="outline-text-5" id="text-org83dd556">
<p>
A performative installation where the participant explores physical manifestations of graphs displaying environment data.
</p>
</div>
</li>
<li><a id="orgc7631c8"></a><a href="https://robsweere.com/2012/01/28/styx-2/">STYX – Rob Sweere</a><br />
<div class="outline-text-5" id="text-orgc7631c8">
<p>
A art peice part of the Hinterland Projects where people places a coin on their head and fixed their gaze on the sky.
</p>
</div>
</li>

<li><a id="orgda2ecc8"></a><a href="http://www.stanza.co.uk/sensity/">Sensity by Stanza</a><br />
<div class="outline-text-5" id="text-orgda2ecc8">
<p>
A series of artworks based on monitoring city spaces. The results are visualisations and sonifications of environmental data made using custom built wireless sensor networks.
</p>
</div>
</li>
<li><a id="orgd967fcc"></a><a href="https://www.media.mit.edu/projects/tidmarsh-living-observatory-portal/overview/">Tidmarsh Living Observatory Portal — MIT Media Lab</a><br />
<div class="outline-text-5" id="text-orgd967fcc">
<p>
A research project that focuses on the design and fabrication of a pavilion that will generate an immersive physical telepresence experience of a natural wetland.
</p>
</div>
</li>
<li><a id="org909ce0b"></a><a href="https://www.media.mit.edu/projects/haptic-footprint/overview/">Haptic Footprint — MIT Media Lab</a><br />
<div class="outline-text-5" id="text-org909ce0b">
<blockquote>
<p>
Can we augment a stroll through nature with sensory experiences usually outside the range of our perception? Haptic Footprints explore using vibrotactile rendering for this purpose."
</p>
</blockquote>
</div>
</li>
<li><a id="org89d4908"></a><a href="https://www.media.mit.edu/projects/living-observatory-sensor-networks-for-documenting-and-experiencing-ecology/overview/">Living Observatory: Sensor networks for documenting and experiencing ecology — MIT Media Lab</a><br />
<div class="outline-text-5" id="text-org89d4908">
<blockquote>
<p>
Living Observatory is an initiative for documenting and interpreting ecological change that will allow people, individually and collectively,
to better understand relationships between ecological processes, human lifestyle choices, and climate change adaptation."
The overarching project of the MIT Responsive Environment Lab in 2018, other projects from it are found here.
</p>
</blockquote>
</div>
</li>

<li><a id="org3abbf55"></a><a href="https://www.media.mit.edu/projects/low-power-wireless-environmental-sensor-node/overview/">Low-power wireless environmental sensor network — MIT Media Lab</a><br />
<div class="outline-text-5" id="text-org3abbf55">
<p>
The environmental sensor network behind many of the MIT Responsive Environments Tidmarsh project.
</p>
</div>
</li>
<li><a id="org5cd153c"></a><a href="https://www.media.mit.edu/projects/doppelmarsh-cross-reality-environmental-sensor-data-browser/overview/">Doppelmarsh: Cross-reality environmental sensor data browser — MIT Media Lab</a><br />
<div class="outline-text-5" id="text-org5cd153c">
<blockquote>
<p>
Doppelmarsh is a cross-reality sensor data browser built for experimenting with presence and multimodal sensory experiences. 
Built on evolving terrain data from a physical wetland landscape, the software integrates real-time data from an environmental sensor network with real-time audio streams 
and other media from the site. Sensor data is rendered in the scene in both visual representations and as 3D sonification. 
Users can explore this data by walking on the virtual terrain in a first person view, or flying high above it. This flexibility allows Doppelmarsh to serve as an interface 
to other research platforms on the site, such as Quadrasense, an augmented reality UAV system that blends a flying live camera view with a virtual camera from Doppelmarsh. 
We are currently investigating methods for representing subsurface data, such as soil and water temperatures at depth, as well as automation in scene and terrain painting.
</p>
</blockquote>
</div>
</li>
<li><a id="orgcff88b5"></a><a href="https://www.media.mit.edu/projects/listentree-audio-haptic-display-in-the-natural-environment/overview/">Overview ‹ ListenTree: Audio-haptic display in the natural environment — MIT Media Lab</a><br />
<div class="outline-text-5" id="text-orgcff88b5">
<p>
A tree that conducts sounds coming from a remote wetland. 
</p>
<blockquote>
<p>
Our intervention is motivated by a need for forms of display that fade into the background, inviting attention rather than requiring it. We consume most digital information through devices that alienate us from our surroundings;
ListenTree points to a future where digital information might become enmeshed in material.
</p>
</blockquote>
</div>
</li>
</ul>
</div>
</div>


<div id="outline-container-orge173c5c" class="outline-3">
<h3 id="orge173c5c">Technology and Fabrication</h3>
<div class="outline-text-3" id="text-orge173c5c">
</div>
<div id="outline-container-org78969d5" class="outline-4">
<h4 id="org78969d5">Sensing Sub-System</h4>
<div class="outline-text-4" id="text-org78969d5">
</div>
<ul class="org-ul">
<li><a id="org777a6cf"></a>Microcontroller<br />
<div class="outline-text-5" id="text-org777a6cf">
<ul class="org-ul">
<li>low cost</li>
<li>low energy</li>
</ul>
</div>
</li>

<li><a id="orgd636c8e"></a>Connectivity<br /></li>
<li><a id="org46320a3"></a>Atmospheric sensors<br /></li>

<li><a id="org0e7a6b4"></a>Soil sensors<br /></li>

<li><a id="org25d1ada"></a>Colour sensor<br /></li>
</ul>
</div>


<div id="outline-container-org0eed8a2" class="outline-4">
<h4 id="org0eed8a2">Data Pipeline Sub-System</h4>
</div>


<div id="outline-container-org7feb28d" class="outline-4">
<h4 id="org7feb28d">Remote Ubication Expression Sub-System (RUESS)</h4>
<div class="outline-text-4" id="text-org7feb28d">
<ul class="org-ul">
<li>Interactive</li>
<li>Informing</li>
<li>Connecting</li>
<li>Expressive</li>
<li>Tactile</li>
</ul>
</div>

<ul class="org-ul">
<li><a id="orgd74b392"></a>3D printing<br /></li>
</ul>
</div>
</div>


<div id="outline-container-org647c990" class="outline-3">
<h3 id="org647c990">Development and Planning</h3>
<div class="outline-text-3" id="text-org647c990">
</div>
<div id="outline-container-orgbe2bb3e" class="outline-4">
<h4 id="orgbe2bb3e">Timeline and Milestones</h4>
</div>


<div id="outline-container-org3bcb927" class="outline-4">
<h4 id="org3bcb927">Potential Issues and Contingencies</h4>
</div>


<div id="outline-container-org9b5394f" class="outline-4">
<h4 id="org9b5394f">Required Knowledge</h4>
</div>


<div id="outline-container-org050fcc6" class="outline-4">
<h4 id="org050fcc6">Required Learning</h4>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Bevis</p>
<p class="date">Created: 2019-12-02 Mon 02:19</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
